{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA7\n",
    "### Ian Myers\n",
    "### Class: CPSC 322-02, Fall 2024\n",
    "### Programming Assignment #7\n",
    "### 11/11/24\n",
    "### Description: This program uses Decision Trees, Naive bayes, knn and dummy classifiers to test precision and other metrics\n",
    "The bonus was not attempted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "madness_data = MyPyTable()\n",
    "filename = os.path.join(\"input_data\", \"tournament_games2016-2021.csv\")\n",
    "madness_data.load_from_file(filename)\n",
    "rows, cols = madness_data.get_shape()\n",
    "\n",
    "# Assign X and y train and test for tournament seed only\n",
    "X_seed = [[val] for val in madness_data.get_column(\"TournamentSeed\")]\n",
    "X_subset = [val for val in madness_data.get_columns([\"LastOrdinalRank\", \"RegularSeasonFGPercentMean\", \"RegularSeasonAwayPercentageWon\"])]\n",
    "y = [val for val in madness_data.get_column(\"Winner\")]\n",
    "\n",
    "\n",
    "# Set up naive bayes, knn and dummy, and decision tree\n",
    "naive_clf = MyNaiveBayesClassifier()\n",
    "knn_clf = MyKNeighborsClassifier(n_neighbors=5, categorical=True)\n",
    "dum_clf = MyDummyClassifier()\n",
    "tree_clf = MyDecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tournament Seed only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "STEP 1: Accuracy and error rate\n",
      "===========================================\n",
      "Decision Tree Classifier: accuracy = 0.69  error rate = 0.31\n",
      "Naive Bayes Classifier: accuracy = 0.69  error rate = 0.31\n",
      "k Nearest Neighbors Classifier: accuracy = 0.69  error rate = 0.31\n",
      "Dummy Classifier: accuracy = 0.52  error rate = 0.48\n",
      "\n",
      "===========================================\n",
      "STEP 2: Precision, recall, and F1 measure\n",
      "===========================================\n",
      "Decision Tree Classifier: precision = 0.68  recall = 0.76  F1 = 0.72\n",
      "Naive Bayes Classifier: precision = 0.68  recall = 0.76  F1 = 0.72\n",
      "k Nearest Neighbors Classifier: precision = 0.68  recall = 0.76  F1 = 0.72\n",
      "Dummy Classifier: precision = 0.52  recall = 1.0  F1 = 0.69\n",
      "\n",
      "===========================================\n",
      "STEP 3: Confusion Matrix\n",
      "===========================================\n",
      "Decision Tree Classifier:\n",
      "Winner      H    A    total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         133   42      175                 76\n",
      "A          62   97      159                 61\n",
      "Naive Bayes Classifier:\n",
      "Winner      H    A    total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         133   42      175                 76\n",
      "A          62   97      159                 61\n",
      "k Nearest Neighbors Classifier:\n",
      "Winner      H    A    total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         120   55      175                 69\n",
      "A          52  107      159                 67\n",
      "Dummy Classifier:\n",
      "Winner      H    A    total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         175    0      175                100\n",
      "A         159    0      159                  0\n"
     ]
    }
   ],
   "source": [
    "# Cross val predict\n",
    "tree_acc, tree_error, tree_precision, tree_recall, tree_f1 = myevaluation.cross_val_predict(tree_clf, X_seed, y, 10)\n",
    "naive_acc, naive_error, naive_precision, naive_recall, naive_f1 = myevaluation.cross_val_predict(naive_clf, X_seed, y, 10)\n",
    "knn_acc, knn_error, knn_precision, knn_recall, knn_f1 = myevaluation.cross_val_predict(knn_clf, X_seed, y, 10)\n",
    "dum_acc, dum_error, dum_precision, dum_recall, dum_f1 = myevaluation.cross_val_predict(dum_clf, X_seed, y, 10)\n",
    "\n",
    "# Print Results\n",
    "print(\"===========================================\")\n",
    "print(\"STEP 1: Accuracy and error rate\")\n",
    "print(\"===========================================\")\n",
    "print(\"Decision Tree Classifier: accuracy =\", round(tree_acc,2), \" error rate =\", round(tree_error,2))\n",
    "print(\"Naive Bayes Classifier: accuracy =\", round(naive_acc,2), \" error rate =\", round(naive_error,2))\n",
    "print(\"k Nearest Neighbors Classifier: accuracy =\", round(knn_acc,2), \" error rate =\", round(knn_error,2))\n",
    "print(\"Dummy Classifier: accuracy =\", round(dum_acc,2), \" error rate =\", round(dum_error,2))\n",
    "print()\n",
    "print(\"===========================================\")\n",
    "print(\"STEP 2: Precision, recall, and F1 measure\")\n",
    "print(\"===========================================\")\n",
    "print(\"Decision Tree Classifier: precision =\", round(tree_precision,2), \" recall =\", round(tree_recall,2), \" F1 =\", round(tree_f1,2))\n",
    "print(\"Naive Bayes Classifier: precision =\", round(naive_precision,2), \" recall =\", round(naive_recall,2), \" F1 =\", round(naive_f1,2))\n",
    "print(\"k Nearest Neighbors Classifier: precision =\", round(knn_precision,2), \" recall =\", round(knn_recall,2), \" F1 =\", round(knn_f1,2))\n",
    "print(\"Dummy Classifier: precision =\", round(dum_precision,2), \" recall =\", round(dum_recall,2), \" F1 =\", round(dum_f1,2))\n",
    "print()\n",
    "print(\"===========================================\")\n",
    "print(\"STEP 3: Confusion Matrix\")\n",
    "print(\"===========================================\")\n",
    "labels = [\"H\", \"A\"]\n",
    "print(\"Decision Tree Classifier:\")\n",
    "myevaluation.pretty_print_confusion_matrix(tree_clf, X_seed, y, labels, \"Winner\", k_sub_samples=10)\n",
    "print(\"Naive Bayes Classifier:\")\n",
    "myevaluation.pretty_print_confusion_matrix(naive_clf, X_seed, y, labels, \"Winner\", k_sub_samples=10)\n",
    "print(\"k Nearest Neighbors Classifier:\")\n",
    "myevaluation.pretty_print_confusion_matrix(knn_clf, X_seed, y, labels, \"Winner\", k_sub_samples=10)\n",
    "print(\"Dummy Classifier:\")\n",
    "myevaluation.pretty_print_confusion_matrix(dum_clf, X_seed, y, labels, \"Winner\", k_sub_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Feature Subset: \n",
    "* LastOrdinalRank \n",
    "* RegularSeasonFGPercentMean\n",
    "* RegularSeasonAwayPercentageWon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "STEP 1: Accuracy and error rate\n",
      "===========================================\n",
      "Decision Tree Classifier: accuracy = 0.69  error rate = 0.31\n",
      "Naive Bayes Classifier: accuracy = 0.66  error rate = 0.34\n",
      "k Nearest Neighbors Classifier: accuracy = 0.66  error rate = 0.34\n",
      "Dummy Classifier: accuracy = 0.52  error rate = 0.48\n",
      "\n",
      "===========================================\n",
      "STEP 2: Precision, recall, and F1 measure\n",
      "===========================================\n",
      "Decision Tree Classifier: precision = 0.71  recall = 0.7  F1 = 0.7\n",
      "Naive Bayes Classifier: precision = 0.67  recall = 0.7  F1 = 0.69\n",
      "k Nearest Neighbors Classifier: precision = 0.62  recall = 0.72  F1 = 0.67\n",
      "Dummy Classifier: precision = 0.52  recall = 1.0  F1 = 0.69\n",
      "\n",
      "===========================================\n",
      "STEP 3: Confusion Matrix\n",
      "===========================================\n",
      "Decision Tree Classifier:\n",
      "Winner      H    A    total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         122   53      175                 70\n",
      "A          51  108      159                 68\n",
      "Naive Bayes Classifier:\n",
      "Winner      H    A    total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         128   47      175                 73\n",
      "A          64   95      159                 60\n",
      "k Nearest Neighbors Classifier:\n",
      "Winner      H    A    total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         145   30      175                 83\n",
      "A         106   53      159                 33\n",
      "Dummy Classifier:\n",
      "Winner      H    A    total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         175    0      175                100\n",
      "A         159    0      159                  0\n"
     ]
    }
   ],
   "source": [
    "# Cross val predict\n",
    "tree_acc, tree_error, tree_precision, tree_recall, tree_f1 = myevaluation.cross_val_predict(tree_clf, X_subset, y, 10)\n",
    "naive_acc, naive_error, naive_precision, naive_recall, naive_f1 = myevaluation.cross_val_predict(naive_clf, X_subset, y, 10)\n",
    "knn_acc, knn_error, knn_precision, knn_recall, knn_f1 = myevaluation.cross_val_predict(knn_clf, X_subset, y, 10)\n",
    "dum_acc, dum_error, dum_precision, dum_recall, dum_f1 = myevaluation.cross_val_predict(dum_clf, X_subset, y, 10)\n",
    "\n",
    "# Print Results\n",
    "print(\"===========================================\")\n",
    "print(\"STEP 1: Accuracy and error rate\")\n",
    "print(\"===========================================\")\n",
    "print(\"Decision Tree Classifier: accuracy =\", round(tree_acc,2), \" error rate =\", round(tree_error,2))\n",
    "print(\"Naive Bayes Classifier: accuracy =\", round(naive_acc,2), \" error rate =\", round(naive_error,2))\n",
    "print(\"k Nearest Neighbors Classifier: accuracy =\", round(knn_acc,2), \" error rate =\", round(knn_error,2))\n",
    "print(\"Dummy Classifier: accuracy =\", round(dum_acc,2), \" error rate =\", round(dum_error,2))\n",
    "print()\n",
    "print(\"===========================================\")\n",
    "print(\"STEP 2: Precision, recall, and F1 measure\")\n",
    "print(\"===========================================\")\n",
    "print(\"Decision Tree Classifier: precision =\", round(tree_precision,2), \" recall =\", round(tree_recall,2), \" F1 =\", round(tree_f1,2))\n",
    "print(\"Naive Bayes Classifier: precision =\", round(naive_precision,2), \" recall =\", round(naive_recall,2), \" F1 =\", round(naive_f1,2))\n",
    "print(\"k Nearest Neighbors Classifier: precision =\", round(knn_precision,2), \" recall =\", round(knn_recall,2), \" F1 =\", round(knn_f1,2))\n",
    "print(\"Dummy Classifier: precision =\", round(dum_precision,2), \" recall =\", round(dum_recall,2), \" F1 =\", round(dum_f1,2))\n",
    "print()\n",
    "print(\"===========================================\")\n",
    "print(\"STEP 3: Confusion Matrix\")\n",
    "print(\"===========================================\")\n",
    "labels = [\"H\", \"A\"]\n",
    "print(\"Decision Tree Classifier:\")\n",
    "myevaluation.pretty_print_confusion_matrix(tree_clf, X_subset, y, labels, \"Winner\", k_sub_samples=10)\n",
    "print(\"Naive Bayes Classifier:\")\n",
    "myevaluation.pretty_print_confusion_matrix(naive_clf, X_subset, y, labels, \"Winner\", k_sub_samples=10)\n",
    "print(\"k Nearest Neighbors Classifier:\")\n",
    "myevaluation.pretty_print_confusion_matrix(knn_clf, X_subset, y, labels, \"Winner\", k_sub_samples=10)\n",
    "print(\"Dummy Classifier:\")\n",
    "myevaluation.pretty_print_confusion_matrix(dum_clf, X_subset, y, labels, \"Winner\", k_sub_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "Looking at the results of both the tournament seed only and my chosen feature subset, it seems that the Decision Tree classifier is one of the best classifiers in overall quality, but is fairly close in metrics to the Naive Bayes classifier. In the first subset, Decision Tree and Naive Bayes are exactly the same in metrics but in my unique subset, it seems the Decision Tree has slightly higher metrics. Decision tree is the highest in almost every metric or close to the top if not. Overall, the Decision Tree classifier seems like a strong classifier to have in ones tool kit when predicting something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out Decision Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF LastOrdinalRank == H AND RegularSeasonAwayPercentageWon == H AND RegularSeasonFGPercentMean == A THEN Winner = A\n",
      "IF LastOrdinalRank == H AND RegularSeasonAwayPercentageWon == A AND RegularSeasonFGPercentMean == A THEN Winner = A\n",
      "IF LastOrdinalRank == H AND RegularSeasonAwayPercentageWon == H AND RegularSeasonFGPercentMean == H THEN Winner = A\n",
      "IF LastOrdinalRank == H AND RegularSeasonAwayPercentageWon == A AND RegularSeasonFGPercentMean == H THEN Winner = A\n",
      "IF LastOrdinalRank == A AND RegularSeasonFGPercentMean == A AND RegularSeasonAwayPercentageWon == H THEN Winner = H\n",
      "IF LastOrdinalRank == A AND RegularSeasonFGPercentMean == A AND RegularSeasonAwayPercentageWon == A THEN Winner = H\n",
      "IF LastOrdinalRank == A AND RegularSeasonFGPercentMean == H AND RegularSeasonAwayPercentageWon == H THEN Winner = H\n",
      "IF LastOrdinalRank == A AND RegularSeasonFGPercentMean == H AND RegularSeasonAwayPercentageWon == A THEN Winner = H\n"
     ]
    }
   ],
   "source": [
    "tree_clf.fit(X_subset, y)\n",
    "tree_clf.print_decision_rules(attribute_names=[\"LastOrdinalRank\", \"RegularSeasonFGPercentMean\", \"RegularSeasonAwayPercentageWon\"],class_name=\"Winner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Reflection:\n",
    "The decision tree rules seem to be heavily dependent on what the value of the last ordinal rank is compared to the other attributes in the subset. With this knowledge we could likely prune the other parts of the tree so that they are dependent on solely the LastOrdinalRank. This would provide similar if not greater accuracy. The new decision rules would be the following.\\\\\n",
    "IF LastOrdinalRank == H THEN Winner = A\\\\\n",
    "IF LastOrdinalRank == A THEN Winner = H\\\\\n",
    "This would prune the tree for a better or similar accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
